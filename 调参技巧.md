
## 大batch数据训练技巧
**1.	Linear scaling learning rate**


增大batch size不会改变随机梯度的期望，但是会减少它的方差。因此可以考虑沿着梯度相反的方向线性增大学习率。
当batch_size（b）=256时lr=0.1，若改变，则lr=0.1 × b/256.（适用于小批量数据训练）

**2.	Learning rate warmup**

训练过程中，参数都是随机选择的，因此会明显偏离最优解。使用过大的学习率，会使模型不稳定，使用过小的学习率，训练时间加长，而且容易得到局部最优解（鞍点）。
预热学习率：学习率从0到预设定的学习率lr逐步上升，若预热epoch=m，第i个batch，则学习率每次提高i*lr/m

**3.	Zero γ**

批归一化 (BN)首先对输入数据x做归一化操作，即 x_norm = (x-u)/std，然后对归一化后的x进行比例缩放和位移 y=α× x_norm + β，（即caffe中的scale层）。一般情况下，α和β分别初始化为1和0。这里BN层γ=0将所有跟在残差块后的BN层的全部初始化为0，即网络进过scale后返回原始数据，网络参数层变少，使得网络在开始阶段变的更容易训练

**4.	no bias decay**

权重衰减(weight decay)广泛应用于weights 和 bias中，一定程度上减少模型过拟合的问题。bias无衰减仅仅在卷积和全连接层的权重中使用权重衰减。BN层的α和β不使用权重衰减。
**低精度训练**

## 模型Tweaks
**不同步长的resnet模型**

**Resnet-B**

Path A的第一个1 x 1卷积层使用了strde=2的步长，会导致3/4的信息丢失。因此ResNet-B将第一二个卷积层的步长交换。

**Resnet-C**

因为卷积的代价会随着卷积核的长和宽增大而接近平方增加，因此ResNet-C使用3个连续的3 x 3 卷积替换Input stem的7 x 7 卷积

**Resnet-D**

ResNet-D在ResNet-B的基础上进一步调整，在Path B的1 x 1卷积前面，加入2 x 2 stride 2的pooling层，将下采样提前，避免了3/4的信息丢失。
## 训练改进

**1．	余弦学习率衰减**

总批次数为T，初始化学习率为η ，在第t批时的学习率ηt为：
 
**2．	标签平滑**

分类网络最后的全连接层softmax存在过拟合风险，标签平滑修改原来的softmax函数为
 
预测标签和原始标签使用（4）公式计算得到新的标签，然后利用新的标签与原来标签计算KL散度（5公式则为计算z）

**3．	知识蒸馏**

通过引入与教师网络（teacher network：复杂、但推理性能优越）相关的软目标（soft-target）作为total loss的一部分，以诱导学生网络（student network：精简、低复杂度）的训练，实现知识迁移（knowledge transfer）
 
其中p为样本概率分布，z和r分别为学生和教师网络的输出，l(p; softmax(z)) 为负交叉熵损失，T为超参数。

**4．	混合训练**
 
混合训练每次从随机抽取的两个样本 (xi, yi) 和 (xj, yj)中通过线性权值组合形成一个新样本
其中，0 ≤ \le≤ λ ≤ \le≤ 1服从 Beta(α, α) 分布，混合训练只使用新生成的样本训练网络
