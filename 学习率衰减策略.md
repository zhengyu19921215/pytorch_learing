## 学习率衰减六大策略(pytorch)

以下学习率lr=0.1，学习率调整倍数为 gamma 倍，均调用torch.optim. lr_scheduler库函数

**1. 等间隔调整学习率 StepLR**


调用函数： StepLR(optimizer, step_size, gamma=0.1, last_epoch=-
方法：调整间隔为 step_size，list形式。间隔单位是epoch，若step_size=30，则每30个epoch下降学习率*0.1
**2.按需调整学习率**


调用函数：MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)
milestones(list)- 一个 list，每一个元素代表何时调整学习率。若meilestones=[10,30,50]，则在第10,30,50个epoch时，学习率*0.1，0.01,0.001,0.0001.
**3. 指数衰减调整学习率**


调用函数：ExponentialLR(optimizer, gamma, last_epoch=-1)
方法：lr=lr∗gamma∗∗epoch ，逐epoch下降
**4.余弦退火调整学习率**


调用函数： CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)
方法：以余弦函数为周期，并在每个周期最大值时重新设置学习率。以初始学习率为最大学习率，以 2∗Tmax 为周期，在一个周期内先下降，后上升。eta_min为周期内最小学习率
**5.自适应调整学习率**


调用函数： ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)
方法：在patience个epoch内mode模式下，loss不下降或者正确率不上升，开始降低学习率，eps衰减最小值
**6.自定义调解学习率**


调用函数：LambdaLR(optimizer, lr_lambda, last_epoch=-1)
lr=base_lr∗lmbda(self.last_epoch)
调整方法：lr_lambda(function or list)- 一个计算学习率调整倍数的函数，输入通常为 step，当有多个参数组时，设为 list。
